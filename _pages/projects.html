---
layout: default
title: Projects | Aditya Pratap Singh
description: Complete portfolio of AI engineering projects including LLM systems, text-to-3D generation, user persona analysis, and research-oriented ML systems.
sitemap: true
permalink: /projects/
---

<div style="margin-top: 30px; margin-bottom: 30px;" class="projects-page">
<div style="max-width: 2500px; margin: 0 auto; padding: 0 20px;">
<div style="width: 100%;">
<h1 style="font-weight: 700; font-size: 2.5rem; color: #222; margin-bottom: 20px; text-align: center;">Projects</h1>
<p style="font-size: 1.1rem; line-height: 1.7; color: #444; margin-bottom: 0; text-align: center; max-width: 800px; margin-left: auto; margin-right: auto;">This page documents my applied AI engineering and research projects, spanning LLM systems, retrieval-augmented generation, model compression, and production-grade ML pipelines.</p>
</div>
</div>
</div>

<!-- Filter Buttons -->
<div style="text-align: center; margin-bottom: 40px;">
<div style="max-width: 2500px; margin: 0 auto; padding: 0 20px;">
<div style="width: 100px; height: 1px; background-color: #000; margin: 0 auto 20px;"></div>
<div id="filter-buttons" style="display: flex; justify-content: center; align-items: center; gap: 20px;">
<button class="filter-btn active" data-filter="all" style="padding: 8px 20px; border: 1px solid #000; background: #000; color: white; cursor: pointer; font-family: 'Geist', sans-serif; font-weight: 500; border-radius: 4px;">All Projects</button>
<button class="filter-btn" data-filter="completed" style="padding: 8px 20px; border: 1px solid #000; background: white; color: #000; cursor: pointer; font-family: 'Geist', sans-serif; font-weight: 500; border-radius: 4px;">Completed</button>
<button class="filter-btn" data-filter="ongoing" style="padding: 8px 20px; border: 1px solid #000; background: white; color: #000; cursor: pointer; font-family: 'Geist', sans-serif; font-weight: 500; border-radius: 4px;">Ongoing</button>
</div>
</div>
</div>

<style>
/* CRITICAL: Override Bootstrap layout for projects page */
.projects-page .container-fluid,
.projects-page > .container-fluid,
body .projects-page .container-fluid {
  max-width: 3000px !important;
  width: 100% !important;
  padding: 0 !important;
}

.projects-page .row,
.projects-page > .row,
body .projects-page .row {
  max-width: 100% !important;
  width: 100% !important;
  margin: 0 !important;
  padding: 0 !important;
}

/* Independent layout - no dependency on SHB_css.scss */
.projects-page {
  width: 100%;
  background: transparent;
}

/* Custom container with full control */
.projects-container {
  max-width: 3000px;
  margin: 0 auto;
  padding: 0 20px;
  width: 100%;
}

/* Center content wrapper */
.content-wrapper {
  width: 100%;
  max-width: 3000px;
  margin: 0 auto;
  padding: 0 20px;
}

/* Project cards with custom layout */
.project-card {
  background: #fafafa;
  border: 1px solid #e0e0e0;
  border-radius: 8px;
  padding: 25px;
  margin: 30px auto;
  box-shadow: 0 2px 4px rgba(0,0,0,0.05);
  max-width: 3000px;
  width: 100%;
  position: relative;
  overflow: hidden;
  transition: opacity 0.3s ease;
}

.project-card.hidden {
  display: none;
}

/* Custom grid layout without Bootstrap - removed for consistency */
/* All projects now use Bootstrap .jumbotron .row .col-md-4 structure */

/* Ensure Bootstrap columns have consistent sizing */
.jumbotron .col-md-4 {
  max-width: 33.33%;
  flex: 0 0 33.33%;
}

/* Images */
.project-image-column img,
.project-image-column .image-placeholder {
  width: 100%;
  height: 300px;
  object-fit: cover;
  border-radius: 8px;
  margin-bottom: 15px;
}

.image-placeholder {
  background-color: #e9ecef;
  display: flex;
  align-items: center;
  justify-content: center;
  color: #6c757d;
  font-size: 0.8rem;
}

/* Typography */
.project-title {
  font-weight: 700;
  font-size: 1.4rem;
  color: #222;
  margin-bottom: 12px;
}

.project-subtitle {
  font-weight: 600;
  font-size: 1.1rem;
  color: #222;
  margin-bottom: 10px;
  font-style: italic;
}

.project-text {
  font-size: 0.9rem;
  line-height: 1.5;
  color: #444;
  margin-bottom: 12px;
}

.project-meta {
  color: #6c757d;
  font-size: 0.8rem;
  margin-bottom: 10px;
  text-align: center;
}

.project-description {
  font-size: 0.85rem;
  font-style: italic;
  color: #555;
  text-align: center;
  margin-bottom: 0;
}

/* Lists */
.project-list {
  padding-left: 20px;
  margin-bottom: 15px;
  font-size: 0.85rem;
  line-height: 1.5;
  color: #444;
}

/* Expand/collapse functionality */
.project-card.collapsed .card-content {
  max-height: 400px;
  overflow: hidden;
  position: relative;
}

.project-card.collapsed .card-content::after {
  content: '';
  position: absolute;
  bottom: 0;
  left: 0;
  right: 0;
  height: 80px;
  background: linear-gradient(transparent, rgba(250, 250, 250, 0.9));
  pointer-events: none;
}

.card-content {
  transition: max-height 0.5s ease-in-out;
  overflow: hidden;
}

.see-more-container {
  text-align: center;
  margin-top: 20px;
  position: relative;
  z-index: 10;
  padding-bottom: 15px;
}

.see-more-text {
  color: #000;
  cursor: pointer;
  font-family: 'Geist', sans-serif;
  font-weight: 500;
  font-size: 0.95rem;
  text-decoration: underline;
  text-underline-offset: 4px;
  text-decoration-thickness: 1px;
  display: inline-block;
  transition: all 0.3s ease;
}

.see-more-text:hover {
  color: #333;
}

.arrow-down {
  width: 0;
  height: 0;
  border-left: 4px solid transparent;
  border-right: 4px solid transparent;
  border-top: 6px solid #000;
  margin: 8px auto 0;
  transition: transform 0.3s ease;
}

.project-card.expanded .arrow-down {
  transform: rotate(180deg);
}

/* Filter buttons */
.filter-btn:hover {
  background-color: #f8f9fa !important;
}

.filter-btn.active {
  background-color: #000 !important;
  color: white !important;
}

/* Carousel styles */
.image-carousel {
  position: relative;
  width: 100%;
  margin-bottom: 15px;
}

.main-image {
  width: 100%;
  height: auto;
  max-height: 300px;
  object-fit: contain;
  border-radius: 8px;
  background-color: #f8f9fa;
}

.thumbnail-container {
  display: flex;
  gap: 8px;
  margin-top: 10px;
  overflow-x: auto;
  padding: 5px 0;
  position: relative;
}

.thumbnail-controls {
  display: flex;
  align-items: center;
  gap: 8px;
  width: 100%;
}

.thumbnail-scroll {
  display: flex;
  gap: 6px;
  overflow-x: auto;
  scroll-behavior: smooth;
  flex: 1;
  padding: 5px 0;
  justify-content: flex-start;
  scrollbar-width: thin;
  -webkit-overflow-scrolling: touch;
}

.thumbnail-scroll::-webkit-scrollbar {
  height: 4px;
}

.thumbnail-scroll::-webkit-scrollbar-track {
  background: #f1f1f1;
  border-radius: 2px;
}

.thumbnail-scroll::-webkit-scrollbar-thumb {
  background: #c1c1c1;
  border-radius: 2px;
}

.thumbnail-scroll::-webkit-scrollbar-thumb:hover {
  background: #a8a8a8;
}

.thumbnail-arrow {
  background: rgba(0,0,0,0.5);
  color: white;
  border: none;
  padding: 6px;
  cursor: pointer;
  border-radius: 4px;
  font-size: 10px;
  flex-shrink: 0;
  display: flex;
  align-items: center;
  justify-content: center;
  min-width: 24px;
  height: 24px;
}

.thumbnail-arrow:hover {
  background: rgba(0,0,0,0.7);
}

.thumbnail-arrow:disabled {
  background: rgba(0,0,0,0.2);
  cursor: not-allowed;
}

.thumbnail {
  width: 50px;
  height: 38px;
  object-fit: cover;
  border-radius: 4px;
  cursor: pointer;
  opacity: 0.6;
  transition: opacity 0.3s ease;
  border: 2px solid transparent;
  flex-shrink: 0;
}

.thumbnail:hover {
  opacity: 1;
}

.thumbnail.active {
  opacity: 1;
  border-color: #007bff;
}

/* Responsive design */
@media (max-width: 768px) {
  .project-layout {
    flex-direction: column;
    gap: 20px;
  }
  
  .project-image-column {
    flex: 1;
    max-width: 100%;
  }
  
  .projects-container,
  .content-wrapper {
    padding: 0 15px;
  }
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const filterButtons = document.querySelectorAll('.filter-btn');
  const projectCards = document.querySelectorAll('.project-card');
  
  // Initialize cards as collapsed
  projectCards.forEach(card => {
    card.classList.add('collapsed');
    const cardContent = card.querySelector('.card-content');
    if (cardContent) {
      cardContent.style.maxHeight = '400px';
      cardContent.style.overflow = 'hidden';
    }
  });
  
  filterButtons.forEach(button => {
    button.addEventListener('click', function() {
      // Remove active class from all buttons
      filterButtons.forEach(btn => {
        btn.classList.remove('active');
        btn.style.backgroundColor = 'white';
        btn.style.color = '#000';
      });
      
      // Add active class to clicked button
      this.classList.add('active');
      this.style.backgroundColor = '#000';
      this.style.color = 'white';
      
      const filter = this.getAttribute('data-filter');
      
      // Show/hide projects based on filter
      projectCards.forEach(card => {
        if (filter === 'all') {
          card.classList.remove('hidden');
        } else if (filter === 'completed' && card.classList.contains('completed')) {
          card.classList.remove('hidden');
        } else if (filter === 'ongoing' && card.classList.contains('ongoing')) {
          card.classList.remove('hidden');
        } else {
          card.classList.add('hidden');
        }
      });
    });
  });
  
  // Handle see more/less functionality
  document.addEventListener('click', function(e) {
    if (e.target.classList.contains('see-more-text') || e.target.classList.contains('arrow-down')) {
      const container = e.target.closest('.see-more-container');
      const card = container.closest('.project-card');
      const seeMoreText = container.querySelector('.see-more-text');
      const cardContent = card.querySelector('.card-content');
      
      if (!cardContent) return;
      
      if (card.classList.contains('collapsed')) {
        // Expand card
        card.classList.remove('collapsed');
        card.classList.add('expanded');
        seeMoreText.textContent = 'See less';
        // Temporarily disable transition to measure full height
        cardContent.style.transition = 'none';
        cardContent.style.maxHeight = 'none';
        const fullHeight = cardContent.scrollHeight;
        // Reset to collapsed height and then animate to full height
        cardContent.style.maxHeight = '400px';
        cardContent.style.overflow = 'hidden';
        // Force reflow
        cardContent.offsetHeight;
        // Enable transition and expand
        cardContent.style.transition = 'max-height 0.5s ease-in-out';
        cardContent.style.maxHeight = fullHeight + 'px';
      } else {
        // Collapse card
        card.classList.remove('expanded');
        card.classList.add('collapsed');
        seeMoreText.textContent = 'See more';
        cardContent.style.transition = 'max-height 0.5s ease-in-out';
        cardContent.style.maxHeight = '400px';
      }
    }
  });

  // Initialize carousel functionality
  function initCarousel(carousel) {
    const mainImage = carousel.querySelector('.main-image');
    const thumbnails = carousel.querySelectorAll('.thumbnail');
    const prevBtn = carousel.querySelector('.thumbnail-arrow.prev');
    const nextBtn = carousel.querySelector('.thumbnail-arrow.next');
    const thumbnailScroll = carousel.querySelector('.thumbnail-scroll');
    
    let currentIndex = 0;
    
    // Set initial active thumbnail
    if (thumbnails.length > 0) {
      thumbnails[0].classList.add('active');
    }
    
    // Thumbnail click handler
    thumbnails.forEach((thumb, index) => {
      thumb.addEventListener('click', () => {
        currentIndex = index;
        updateMainImage();
        updateActiveThumbnail();
        scrollThumbnailIntoView();
      });
    });
    
    // Arrow click handlers
    prevBtn.addEventListener('click', () => {
      if (currentIndex > 0) {
        currentIndex--;
        updateMainImage();
        updateActiveThumbnail();
        scrollThumbnailIntoView();
      }
    });
    
    nextBtn.addEventListener('click', () => {
      if (currentIndex < thumbnails.length - 1) {
        currentIndex++;
        updateMainImage();
        updateActiveThumbnail();
        scrollThumbnailIntoView();
      }
    });
    
    function updateMainImage() {
      const clickedThumb = thumbnails[currentIndex];
      if (clickedThumb) {
        mainImage.src = clickedThumb.src;
        mainImage.alt = clickedThumb.alt;
      }
      updateArrowStates();
    }
    
    function updateActiveThumbnail() {
      thumbnails.forEach((thumb, index) => {
        thumb.classList.toggle('active', index === currentIndex);
      });
    }
    
    function scrollThumbnailIntoView() {
      const activeThumb = thumbnails[currentIndex];
      if (activeThumb && thumbnailScroll) {
        // Use scrollIntoView for smooth scrolling and mouse/touchpad support
        activeThumb.scrollIntoView({
          behavior: 'smooth',
          inline: 'center',
          block: 'nearest'
        });
      }
    }
    
    function updateArrowStates() {
      if (prevBtn) prevBtn.disabled = currentIndex === 0;
      if (nextBtn) nextBtn.disabled = currentIndex === thumbnails.length - 1;
    }
    
    // Initialize arrow states
    updateArrowStates();
  }
  
  // Initialize all carousels on the page
  document.querySelectorAll('.image-carousel').forEach(initCarousel);
  
  // Handle URL parameter for auto-expanding specific project
  function handleProjectParameter() {
    const urlParams = new URLSearchParams(window.location.search);
    const projectId = urlParams.get('project');
    
    if (projectId) {
      const targetProjectId = `project-${projectId}`;
      const targetCard = document.getElementById(targetProjectId);
      
      if (targetCard) {
        // Initialize cards as collapsed first
        projectCards.forEach(card => {
          card.classList.add('collapsed');
          const cardContent = card.querySelector('.card-content');
          if (cardContent) {
            cardContent.style.maxHeight = '400px';
            cardContent.style.overflow = 'hidden';
          }
        });
        
        // Expand the target project
        setTimeout(() => {
          const seeMoreContainer = targetCard.querySelector('.see-more-container');
          if (seeMoreContainer) {
            // Trigger the expand functionality
            const seeMoreText = seeMoreContainer.querySelector('.see-more-text');
            const cardContent = targetCard.querySelector('.card-content');
            
            if (cardContent && seeMoreText) {
              targetCard.classList.remove('collapsed');
              targetCard.classList.add('expanded');
              seeMoreText.textContent = 'See less';
              
              // Temporarily disable transition to measure full height
              cardContent.style.transition = 'none';
              cardContent.style.maxHeight = 'none';
              const fullHeight = cardContent.scrollHeight;
              
              // Reset to collapsed height and then animate to full height
              cardContent.style.maxHeight = '400px';
              cardContent.style.overflow = 'hidden';
              
              // Force reflow
              cardContent.offsetHeight;
              
              // Enable transition and expand
              cardContent.style.transition = 'max-height 0.5s ease-in-out';
              cardContent.style.maxHeight = fullHeight + 'px';
            }
          }
          
          // Smooth scroll to the project
          targetCard.scrollIntoView({
            behavior: 'smooth',
            block: 'start'
          });
        }, 500);
      }
    }
  }
  
  // Call the function after page load
  handleProjectParameter();
});
</script>

<!-- Project 1: Tesseract -->
<div class="jumbotron project-card completed" id="project-tesseract" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/tesseract.gif' | relative_url }}" class="main-image" alt="Text-to-3D mesh generation using diffusion models - Tesseract v1">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/tesseract.gif' | relative_url }}" class="thumbnail active" alt="Text-to-3D mesh generation using diffusion models - Tesseract v1">
<img src="https://picsum.photos/seed/tesseract2/50/38.jpg" class="thumbnail" alt="Tesseract project screenshot 2">
<img src="https://picsum.photos/seed/tesseract3/50/38.jpg" class="thumbnail" alt="Tesseract project screenshot 3">
<img src="https://picsum.photos/seed/tesseract4/50/38.jpg" class="thumbnail" alt="Tesseract project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p class="project-meta">August 2025</p>
<p class="project-description">Text-to-3D mesh generation using diffusion models</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 class="project-title">Tesseract v1 - Text-to-3D Mesh Generation System</h3>

<h4 class="project-subtitle">Context & Motivation</h4>
<p class="project-text">I built Tesseract as my first flagship, end-to-end ML system to deliberately move beyond notebook-driven experiments and understand what separates production-grade AI systems from typical college-level projects..</p>
<p class="project-text">Early-stage 3D asset creation is slow and labor-intensive, often forcing artists and developers to start from scratch before any meaningful iteration can begin. While text-to-3D diffusion models exist, most are released as fragile scripts or demos that are difficult to integrate into real workflows, lack reproducibility, and are not designed for deployment or scaling.</p>
<p class="project-text">I wanted to explore how a research model could be wrapped into a reliable, scriptable, and scalable system, usable through real interfaces rather than isolated experiments.</p>

<h4 class="project-subtitle">Problem Statement</h4>
<p class="project-text">How can text-conditioned 3D mesh generation be exposed as a reliable, production-ready system suitable for batch workflows, service-based integration, and reproducible experimentation—rather than remaining a research demo or notebook artifact?</p>

<h4 class="project-subtitle">System Overview</h4>
<p class="project-text">Tesseract v1 is a modular, production-oriented ML pipeline that wraps a diffusion-based 3D generation model and exposes it through multiple interfaces:</p>
<ul class="project-list">
<li><strong>Stateless FastAPI service</strong> for supporting asynchronous, job-based mesh generation</li>
<li><strong>Custom CLI interface</strong> for local usage, batch generation and scripting</li>
<li><strong>Config-driven execution model</strong> (YAML-based) to enable reproducible runs and controlled experimentation</li>
</ul>
<p class="project-text">The system manages prompt ingestion, latent generation, mesh decoding, file export, and optional preview rendering in a unified pipeline. and is is intentionally designed to be stateless at the service layer, device-aware (GPU with CPU fallback), and structured in a way that is compatible with containerized deployment patterns.</p>

<h4 class="project-subtitle">Key Engineering Decisions</h4>
<p class="project-text">Rather than focusing on model novelty, I centered this project around system reliability, iteration ergonomics, and realistic integration constraints:</p>
<ul class="project-list">
<li><strong>Custom CLI with explicit flags</strong> for prompts, batching, output formats, and dry-runs, enabling reproducible experimentation outside notebooks</li>
<li><strong>Batch processing support</strong> to generate multiple samples per prompt, both to improve output selection and to stress-test the pipeline under heavier workloads</li>
<li><strong>Latent cache saving and resume support</strong>, allowing generation to continue from intermediate latent states instead of restarting full runs</li>
<li><strong>Asynchronous job handling</strong> to avoid blocking execution and support concurrent requests</li>
<li><strong>Modular system boundaries</strong> separating API, orchestration, decoding, rendering, and configuration layers</li>
</ul>
<p class="project-text">While initially attempting to extract only the minimal components required for text-to-3D generation, I found that Shape-E's research-oriented codebase was highly coupled, with core functionality spanning a large dependency graph. Rather than risk subtle breakage or silent correctness issues, I chose to vendor the full Shape-E project into the core, and build clean abstraction layers around it instead of aggressively pruning internals.</p>
<p class="project-text">This decision favored correctness, stability, and debuggability over premature modularization.</p>

<h4 class="project-subtitle">Engineering & Stack</h4>
<ul class="project-list">
<li><strong>Core:</strong> Python, PyTorch, Shape-E (text-conditioned diffusion)</li>
<li><strong>Interfaces:</strong> Custom CLI, FastAPI (REST)</li>
<li><strong>Execution:</strong> Async job orchestration, batch processing, latent caching</li>
<li><strong>Configuration:</strong> YAML-driven runtime configuration</li>
<li><strong>Observability:</strong> Structured logging across pipeline stages</li>
<li><strong>Deployment posture:</strong> Designed to be container-friendly and compatible with common Docker and Kubernetes deployment patterns</li>
</ul>
<p class="project-text">The focus was on keeping the system simple to reason about, easy to debug, and reproducible, rather than optimizing prematurely for scale.</p>

<h4 class="project-subtitle">Evaluation & Validation</h4>
<p class="project-text">Evaluation focused on system correctness and operational behavior, rather than benchmark scores:</p>
<ul class="project-list">
<li><strong>Qualitative inspection</strong> of generated meshes to assess usability as starting canvases</li>
<li><strong>Validation of batch execution</strong> under different memory and latency configurations</li>
<li><strong>Failure-mode testing</strong> around decoding, rendering, and partial pipeline crashes</li>
<li><strong>Verification</strong> that cached latents could reliably resume downstream stages</li>
</ul>
<p class="project-text">The emphasis was on ensuring the pipeline behaved predictably under iteration and failure, rather than optimizing for output aesthetics alone.</p>

<h4 class="project-subtitle">Limitations & Reflections</h4>
<p class="project-text">Several engineering choices emerged directly from repeated friction during development.</p>
<p class="project-text">I introduced batch processing not only to increase the likelihood of obtaining useful outputs from stochastic diffusion, but also to intentionally stress-test the pipeline and observe how resource usage and failure modes surfaced under load.</p>
<p class="project-text">Similarly, latent cache saving was added after encountering multiple cases where decoding or rendering failures—often due to small bugs or misconfigurations—forced the entire diffusion process to restart. Persisting intermediate latents significantly reduced wasted compute and made debugging faster and less frustrating.</p>
<p class="project-text">Integrating a research codebase like Shape-E also highlighted the tradeoff between ideal modularity and practical correctness. This project reinforced that, in applied ML systems, stability and recoverability often matter more than architectural purity.</p>
<p class="project-text">As my first flagship, end-to-end ML system, Tesseract helped solidify my understanding of what distinguishes robust engineering from experimental code: careful state management, clear boundaries, and designing for things to fail gracefully.</p>

<h4 class="project-subtitle">Status</h4>
<p class="project-text">Completed (v1) — a production-oriented architecture with clear scope for future improvements in model quality, evaluation rigor, and modular refinement.</p>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>
</div>

<!-- Project 2: Reddit-Persona -->
<div class="jumbotron project-card completed" id="project-reddit-persona" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/reddit.png' | relative_url }}" class="main-image" alt="LLM-based Reddit user persona generation system - Reddit-Persona">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/reddit.png' | relative_url }}" class="thumbnail active" alt="LLM-based Reddit user persona generation system - Reddit-Persona">
<img src="https://picsum.photos/seed/reddit2/50/38.jpg" class="thumbnail" alt="Reddit-Persona project screenshot 2">
<img src="https://picsum.photos/seed/reddit3/50/38.jpg" class="thumbnail" alt="Reddit-Persona project screenshot 3">
<img src="https://picsum.photos/seed/reddit4/50/38.jpg" class="thumbnail" alt="Reddit-Persona project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">July 2025</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">LLM-based user persona generation from Reddit activity</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">Reddit-Persona - LLM-based User Persona Generation</h3>
<div style="margin-bottom: 15px;">
<p class="project-text">I built Reddit-Persona to explore how large language models can synthesize coherent behavioral personas from noisy, unstructured, real-world text data. This project originated as a pre-interview internship assignment, but I intentionally extended it beyond initial requirements to understand how LLMs behave when tasked with higher-level reasoning over fragmented user activity rather than direct question answering.</p>
</div>

<h4 class="project-subtitle">Context & Motivation</h4>
<p class="project-text">I was particularly interested in how design choices around chunking, context density, and output structure affect the quality and interpretability of persona-level outputs.</p>

<h4 class="project-subtitle">Problem Statement</h4>
<p class="project-text">How can we transform a Reddit user's scattered posts and comments—spanning multiple topics, tones, and timeframes—into a structured, interpretable persona, without overwhelming the model or losing important behavioral signals?</p>

<h4 class="project-subtitle">System Overview</h4>
<p class="project-text">Reddit-Persona is a modular pipeline that:</p>
<ul class="project-list">
<li><strong>Scrapes</strong> a user's public Reddit posts and comments using PRAW</li>
<li><strong>Cleans and filters</strong> the data to remove deleted or low-signal content</li>
<li><strong>Splits</strong> activity into topic-consistent text chunks (~2000 characters)</li>
<li><strong>Sends</strong> each chunk to a large language model for persona synthesis</li>
<li><strong>Produces</strong> multiple structured persona blocks, each grounded in a different slice of user behavior</li>
</ul>
<p class="project-text">The system supports both a CLI workflow for scripted runs and a Streamlit UI to make persona generation accessible to non-technical users.</p>

<h4 class="project-subtitle">Key Engineering Decisions</h4>
<p class="project-text">This project emphasized LLM interaction design and reasoning quality, rather than infrastructure scale:</p>
<ul class="project-list">
<li><strong>Chunk-based inference</strong> to preserve topical coherence and avoid diluting behavioral signals in long contexts</li>
<li><strong>Multiple persona generation</strong> (one per chunk) to capture micro-identities instead of forcing a single averaged profile</li>
<li><strong>Use of LLaMA-3.3-70B via Groq</strong> for persona synthesis, chosen for its strong instruction-following behavior and ability to produce structured, multi-facet analyses from unstructured text</li>
<li><strong>Structured persona schema</strong> (traits, motivations, frustrations, goals, supporting quotes) to keep outputs interpretable and auditable</li>
<li><strong>Optional persona ranking logic</strong> to identify the most insight-rich persona block and explain why it was selected</li>
<li><strong>Streamlit UI integration</strong>, added beyond assignment requirements to study how such outputs are consumed in a visual, user-facing workflow</li>
</ul>
<p class="project-text">These decisions were driven by the goal of understanding how to reason with LLM outputs, not just generate text.</p>

<h4 class="project-subtitle">Model Choice Rationale</h4>
<p class="project-text">I chose LLaMA-3.3-70B for this task because persona synthesis requires semantic abstraction, consistency, and narrative grounding, rather than factual recall or short-form completion.</p>
<p class="project-text">Compared to smaller models, LLaMA-70B demonstrated:</p>
<ul class="project-list">
<li>Better stability across long, unstructured inputs</li>
<li>More consistent persona traits across chunks</li>
<li>Stronger adherence to structured output formats</li>
</ul>
<p class="project-text">Using Groq's inference API allowed fast, low-latency processing of multiple chunks per user, making iterative experimentation feasible without compromising output quality. This was particularly important given the chunk-based design of the system.</p>

<h4 class="project-subtitle">Engineering & Stack</h4>
<ul class="project-list">
<li><strong>Language:</strong> Python</li>
<li><strong>Data access:</strong> PRAW (Reddit API Wrapper)</li>
<li><strong>LLM inference:</strong> Groq-hosted LLaMA-3.3-70B</li>
<li><strong>Interfaces:</strong> CLI (main.py) and Streamlit UI (app.py)</li>
<li><strong>Structure:</strong> Modular Python files for scraping, inference, formatting, and orchestration</li>
<li><strong>Environment management:</strong> Conda-based setup for dependency isolation</li>
</ul>
<p class="project-text">The project was intentionally kept simple from an infrastructure perspective to focus on reasoning quality, modularity, and clarity.</p>

<h4 class="project-subtitle">Evaluation & Validation</h4>
<p class="project-text">Evaluation was primarily qualitative and interpretive:</p>
<ul class="project-list">
<li><strong>Comparing persona blocks</strong> across chunks to assess behavioral consistency and diversity</li>
<li><strong>Verifying</strong> that extracted traits and motivations were grounded in explicit Reddit quotes</li>
<li><strong>Observing</strong> how chunk size and ordering affected persona richness and coherence</li>
<li><strong>Testing robustness</strong> against sparse activity, deleted content, and mixed-topic histories</li>
</ul>
<p class="project-text">The emphasis was on interpretability, traceability, and reasoning fidelity, rather than numerical benchmarks.</p>

<h4 class="project-subtitle">Limitations & Reflections</h4>
<p class="project-text">This project surfaced several important lessons.</p>
<p class="project-text">Aggregating all user activity into a single prompt consistently degraded persona quality, reinforcing the importance of chunk-level reasoning. However, generating multiple personas introduced a new ambiguity: deciding which persona to trust. This motivated the addition of an experimental ranking step that re-evaluates persona blocks and selects the most insight-rich candidate, along with an explanation for the choice.</p>
<p class="project-text">From an engineering standpoint, this was my first project where I became comfortable with clean Python project structuring, modular design, and environment isolation using Conda. While it does not yet include advanced logging or configuration separation, it laid the groundwork for how I structure larger systems today.</p>
<p class="project-text">Finally, the project highlighted ethical and practical considerations around persona generation from real user data. For this reason, the system is not publicly hosted and requires users to supply their own API keys.</p>

<h4 class="project-subtitle">Status</h4>
<p class="project-text">Completed — a functional, reasoning-centric system that explores LLM-driven behavioral analysis and serves as an early foundation for more advanced applied and research-oriented work.</p>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>

<!-- Project 3: MÍMIR -->
<div class="jumbotron project-card ongoing" id="project-mimir" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/mimir.png' | relative_url }}" class="main-image" alt="Research-oriented LLM system for DevOps incident reasoning - MÍMIR">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/mimir.png' | relative_url }}" class="thumbnail active" alt="Research-oriented LLM system for DevOps incident reasoning - MÍMIR">
<img src="https://picsum.photos/seed/mimir2/50/38.jpg" class="thumbnail" alt="MÍMIR project screenshot 2">
<img src="https://picsum.photos/seed/mimir3/50/38.jpg" class="thumbnail" alt="MÍMIR project screenshot 3">
<img src="https://picsum.photos/seed/mimir4/50/38.jpg" class="thumbnail" alt="MÍMIR project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">2025 - Present</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">Research-oriented LLM system for DevOps incident reasoning</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">MÍMIR - Research-Oriented LLM System</h3>
<div style="margin-bottom: 15px;">
<p style="font-size: 0.9rem; line-height: 1.5; color: #444; margin-bottom: 12px;">Designing a research-oriented LLM system to study retrieval-augmented reasoning and parameter-efficient adaptation under realistic system constraints. Emphasizes reproducible evaluation over application demos.</p>
</div>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Key Features & Implementation:</h4>
<ul style="padding-left: 20px; margin-bottom: 15px; font-size: 0.85rem; line-height: 1.5; color: #444;">
<li><strong>Retrieval-Augmented Reasoning:</strong> Advanced RAG implementation with multiple knowledge sources and fusion strategies.</li>
<li><strong>Parameter-Efficient Adaptation:</strong> LoRA and quantization techniques for resource-constrained deployment.</li>
<li><strong>System-Level Evaluation:</strong> Comprehensive metrics covering accuracy, latency, cost, and robustness.</li>
<li><strong>Reproducible Benchmarks:</strong> Standardized datasets and evaluation protocols for fair comparison.</li>
<li><strong>Production Constraints:</strong> Real-world considerations including memory limits, inference speed, and scalability.</li>
</ul>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Technical Stack:</h4>
<p style="font-size: 0.85rem; color: #444; margin-bottom: 0;">RAG Systems, LoRA, Model Quantization, Evaluation Frameworks, System Optimization</p>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>

<!-- Project 4: SkySentinel-X - Micro-Doppler Target Classification -->
<div class="jumbotron project-card completed" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/skysentinel 1.png' | relative_url }}" class="main-image" alt="SkySentinel-X micro-Doppler target classification system">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/skysentinel 1.png' | relative_url }}" class="thumbnail active" alt="SkySentinel-X micro-Doppler target classification system">
<img src="https://picsum.photos/seed/skysentinel2/50/38.jpg" class="thumbnail" alt="SkySentinel-X project screenshot 2">
<img src="https://picsum.photos/seed/skysentinel3/50/38.jpg" class="thumbnail" alt="SkySentinel-X project screenshot 3">
<img src="https://picsum.photos/seed/skysentinel4/50/38.jpg" class="thumbnail" alt="SkySentinel-X project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">Smart India Hackathon</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">Radar-vision research prototype for bird vs drone discrimination</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">SkySentinel-X — Micro-Doppler Target Classification (Research Prototype)</h3>
<div style="margin-bottom: 15px;">
<p class="project-text">An exploratory radar–vision research system for bird vs drone discrimination using micro-Doppler signatures</p>
</div>

<h4 class="project-subtitle">Research Context & Motivation</h4>
<p class="project-text">SkySentinel-X is a research-oriented prototype built to investigate whether micro-Doppler signatures extracted from FMCW radar signals can be effectively transformed into a visual learning problem for modern deep learning models.</p>
<p class="project-text">Low-altitude airspace monitoring systems frequently struggle to distinguish biological targets (birds) from small UAVs, leading to false alarms and unreliable threat assessment. Traditional radar pipelines rely on handcrafted features and heuristics, which often fail under real-world variability.</p>
<p class="project-text">This project explores a different question: Can time–frequency representations of radar returns (STFT spectrograms) serve as a robust intermediate representation for CNN-based classification of aerial targets?</p>
<p class="project-text">SkySentinel-X was developed as part of the Smart India Hackathon (SIH) problem statement on micro-Doppler–based target classification and successfully cleared the college-level selection round, validating its research direction and technical grounding.</p>

<h4 class="project-subtitle">Research Problem Statement</h4>
<p class="project-text"><strong>Objective:</strong> To experimentally evaluate whether deep convolutional models trained on STFT spectrograms can learn discriminative micro-Doppler patterns that separate birds from drone-like aerial objects.</p>
<p class="project-text"><strong>Key research challenges addressed:</strong></p>
<ul class="project-list">
<li>High intra-class variability in drone rotor configurations</li>
<li>Overlapping radar signatures between birds and UAVs</li>
<li>Limited labeled radar datasets</li>
<li>Class imbalance across target categories</li>
<li>Translating radar signal processing outputs into ML-ready representations</li>
</ul>

<h4 class="project-subtitle">System Overview (Research Pipeline)</h4>
<p class="project-text">SkySentinel-X implements a signal-processing → vision-learning pipeline, designed explicitly for experimentation and evaluation:</p>
<p class="project-text"><strong>Radar Signal Processing</strong></p>
<ul class="project-list">
<li>FMCW radar returns processed using Short-Time Fourier Transform (STFT)</li>
<li>Generation of micro-Doppler spectrograms capturing motion-induced frequency shifts</li>
</ul>
<p class="project-text"><strong>Representation Learning</strong></p>
<ul class="project-list">
<li>Spectrograms treated as image inputs</li>
<li>Transfer learning using ResNet-50</li>
<li>Fine-tuning restricted to later layers to adapt to radar-domain textures</li>
</ul>
<p class="project-text"><strong>Hierarchical Inference</strong></p>
<ul class="project-list">
<li>Multi-class predictions (rotor types, RC plane, bird)</li>
<li>Aggregation into higher-level semantic groups: bird, drone</li>
<li>Probability-weighted final decision with confidence estimation</li>
</ul>
<p class="project-text">This structure prioritizes interpretability and experimental control, not deployment.</p>

<h4 class="project-subtitle">Key Research & Engineering Decisions</h4>
<p class="project-text"><strong>Why STFT Spectrograms?</strong></p>
<ul class="project-list">
<li>Micro-Doppler effects manifest naturally in the time–frequency domain</li>
<li>Spectrograms preserve: Rotor periodicity, Wing-flap harmonics, Motion-induced frequency spread</li>
<li>Enables reuse of mature CNN architectures without handcrafted signal features</li>
</ul>

<p class="project-text"><strong>Why CNN Transfer Learning?</strong></p>
<ul class="project-list">
<li>Limited radar datasets make training from scratch impractical</li>
<li>Pretrained CNNs provide strong low-level feature extractors</li>
<li>Transfer learning allows rapid hypothesis testing under constrained data</li>
</ul>

<p class="project-text"><strong>Handling Dataset Bias</strong></p>
<ul class="project-list">
<li>Stratified splits to preserve class distribution</li>
<li>Class-weighted loss to counter rotor-class dominance</li>
<li>Augmentations designed to preserve spectrogram semantics</li>
</ul>
<p class="project-text">This project intentionally avoids over-engineering and instead focuses on clean experimental design.</p>

<h4 class="project-subtitle">Evaluation & Experimental Results</h4>
<p class="project-text"><strong>Dataset</strong></p>
<ul class="project-list">
<li>813 labeled spectrogram samples</li>
<li>Target categories: Bird, Drone, RC Plane, Long-blade rotor, Short-blade rotor</li>
</ul>
<p class="project-text"><strong>Results</strong></p>
<ul class="project-list">
<li>Overall accuracy: 94.71%</li>
<li>Macro F1-score: 0.9479</li>
</ul>
<p class="project-text"><strong>Observations</strong></p>
<ul class="project-list">
<li>Near-perfect separation of biological vs mechanical targets</li>
<li>Expected confusion between similar drone rotor configurations</li>
<li>Strong generalization despite limited data volume</li>
</ul>
<p class="project-text">These results support the hypothesis that micro-Doppler spectrograms are a viable visual representation for aerial target classification.</p>

<h4 class="project-subtitle">Research Stack</h4>
<p class="project-text"><strong>Signal Processing</strong></p>
<ul class="project-list">
<li>FMCW radar data</li>
<li>STFT-based time–frequency analysis</li>
</ul>
<p class="project-text"><strong>ML & Experimentation</strong></p>
<ul class="project-list">
<li>PyTorch + FastAI</li>
<li>ResNet-50 (transfer learning)</li>
<li>Class-weighted cross-entropy</li>
<li>Early stopping and checkpointing</li>
</ul>
<p class="project-text"><strong>Analysis</strong></p>
<ul class="project-list">
<li>Confusion matrices</li>
<li>Precision/recall/F1 analysis</li>
<li>Training–validation loss tracking</li>
</ul>

<h4 class="project-subtitle">Research Value & Applications</h4>
<p class="project-text">This project is positioned as a research probe, not a finished product.</p>
<p class="project-text"><strong>Potential downstream relevance:</strong></p>
<ul class="project-list">
<li>Radar-based drone intrusion detection</li>
<li>Bird–drone discrimination in aviation safety</li>
<li>Defense and perimeter surveillance research</li>
<li>Foundations for multimodal radar–vision systems</li>
</ul>
<p class="project-text">Its primary contribution is methodological: demonstrating a clean bridge between radar signal processing and deep visual learning.</p>

<h4 class="project-subtitle">Reflections & Learnings</h4>
<p class="project-text">SkySentinel-X represents an important shift in my work toward research-driven system design:</p>
<ul class="project-list">
<li>Learned to reason across signal processing and ML abstraction layers</li>
<li>Gained experience handling real-world class imbalance rigorously</li>
<li>Developed intuition for radar-domain representations</li>
<li>Practiced designing experiments that test hypotheses, not just models</li>
</ul>
<p class="project-text">While currently implemented as a notebook-based research prototype, the pipeline is intentionally structured to be refactored into modular training and inference systems in future iterations.</p>

<h4 class="project-subtitle">Project Status</h4>
<p class="project-text"><strong>Type:</strong> Research prototype</p>
<p class="project-text"><strong>Stage:</strong> Experimental validation complete</p>
<p class="project-text"><strong>Current form:</strong> Jupyter-based exploratory pipeline</p>
<p class="project-text"><strong>Future work:</strong></p>
<ul class="project-list">
<li>Modularization into training/inference components</li>
<li>Larger, more diverse radar datasets</li>
<li>Real-time radar stream integration</li>
<li>Extended aerial target taxonomy</li>
</ul>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>

<!-- Project 5: ORCA v0 - Wearable Computer Vision Glasses Aid -->
<div class="jumbotron project-card completed" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/orca 1.png' | relative_url }}" class="main-image" alt="ORCA wearable computer vision glasses aid system">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/orca 1.png' | relative_url }}" class="thumbnail active" alt="ORCA wearable computer vision glasses aid system">
<img src="https://picsum.photos/seed/orca2/50/38.jpg" class="thumbnail" alt="ORCA project screenshot 2">
<img src="https://picsum.photos/seed/orca3/50/38.jpg" class="thumbnail" alt="ORCA project screenshot 3">
<img src="https://picsum.photos/seed/orca4/50/38.jpg" class="thumbnail" alt="ORCA project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">College Research Project</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">Low-cost wearable computer vision glasses aid</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">ORCA v0 — Wearable Computer Vision Glasses Aid</h3>
<div style="margin-bottom: 15px;">
<p class="project-text">Omni-Purpose Real-time Computer-vision Assistant (College Research Project)</p>
</div>

<h4 class="project-subtitle">Context & Motivation</h4>
<p class="project-text">Assistive smart glasses for visually impaired users exist, but they are typically proprietary, expensive (₹1–3 lakh / ~$1,200–$3,600), and tightly coupled to closed hardware and cloud-based inference.</p>
<p class="project-text">ORCA v0 was built as a college research project to explore a fundamental question: Can modern open-source computer vision models be composed into a low-cost, real-time, wearable vision aid with meaningful assistive capabilities?</p>
<p class="project-text">The project targeted a novel computer vision glasses concept, using commodity hardware and open-source models to prototype a system that could assist users with environmental awareness, navigation, and social interaction, while remaining affordable and privacy-preserving.</p>

<h4 class="project-subtitle">Problem Statement</h4>
<p class="project-text">How can we design a multi-purpose wearable computer vision system that:</p>
<ul class="project-list">
<li>Operates in real time</li>
<li>Supports multiple assistive use cases, not a single task</li>
<li>Runs on low-cost hardware (₹800 ESP32-CAM ≈ $10)</li>
<li>Avoids cloud dependency for privacy and latency</li>
<li>Remains modular enough for experimentation and research</li>
</ul>
<p class="project-text">...rather than being a closed, single-purpose demo or a high-cost commercial device?</p>

<h4 class="project-subtitle">System Overview</h4>
<p class="project-text">ORCA follows a hybrid edge-compute architecture designed for prototyping wearable vision systems:</p>
<ul class="project-list">
<li><strong>ESP32-CAM (~₹800 / ~$10)</strong> captures live video and streams frames wirelessly</li>
<li><strong>A local compute node</strong> (PC / edge system) performs all inference</li>
<li><strong>Multiple computer vision pipelines</strong> run in parallel</li>
<li><strong>Outputs</strong> are delivered via visual overlays and audio feedback</li>
<li><strong>Designed to integrate</strong> with a transparent OLED display for wearable glasses (prototype concept)</li>
</ul>
<p class="project-text">The system intentionally supports multiple use cases, not just one, making it closer to a general-purpose vision aid than a task-specific demo.</p>

<h4 class="project-subtitle">Core Use Cases Enabled</h4>
<p class="project-text">ORCA was designed to support and experiment with:</p>
<ul class="project-list">
<li><strong>Object Detection</strong> — Identifying common objects in the environment (80+ classes)</li>
<li><strong>Face Detection & Recognition</strong> — Identifying known vs unknown individuals with audio alerts</li>
<li><strong>Depth Estimation</strong> — Estimating relative distances for navigation and obstacle awareness</li>
<li><strong>Multi-Modal Assistance</strong> — Combining visual and audio cues to improve situational awareness</li>
</ul>
<p class="project-text">This multi-capability design was deliberate — most college projects focus on one vision task, whereas ORCA explored system-level integration across tasks.</p>

<h4 class="project-subtitle">Key Engineering Decisions</h4>
<p class="project-text"><strong>1. Multi-Model, Task-Specific Pipelines</strong></p>
<p class="project-text">Instead of forcing all tasks into a single model, ORCA uses specialized models per capability:</p>
<ul class="project-list">
<li><strong>YOLOv10 / NanoDet</strong> — real-time object detection</li>
<li><strong>YuNet + S-Face</strong> — face detection and recognition</li>
<li><strong>MiDaS / Depth Anything V2</strong> — monocular depth estimation</li>
</ul>
<p class="project-text">This allowed independent benchmarking of accuracy vs latency, faster iteration and model swapping, and clear separation of concerns across pipelines.</p>

<p class="project-text"><strong>2. Low-Cost Hardware Constraint</strong></p>
<p class="project-text">A strict constraint was using ESP32-CAM instead of high-end sensors or proprietary cameras. This forced realistic engineering trade-offs around frame resolution, network bandwidth, model size and inference speed, and end-to-end latency budgets. This constraint is what makes ORCA meaningful as a wearable assistive CV project, not just a desktop demo.</p>

<p class="project-text"><strong>3. Privacy-Preserving, Local Inference</strong></p>
<p class="project-text">All inference runs locally: No cloud APIs for face recognition or object detection, no external transmission of sensitive visual data, suitable for assistive and accessibility-related contexts. This mirrors real-world constraints in assistive technology design.</p>

<p class="project-text"><strong>4. Quantization for Real-Time Performance</strong></p>
<p class="project-text">Models were tested with INT8 quantization, reducing memory usage by ~40–50% while maintaining usable accuracy. This enabled ~8 FPS end-to-end processing, sub-second latency, and feasibility on consumer-grade hardware.</p>

<h4 class="project-subtitle">Engineering & Technology Stack</h4>
<p class="project-text"><strong>Languages & Frameworks:</strong> Python, OpenCV, PyTorch</p>
<p class="project-text"><strong>Models:</strong> YOLOv10, NanoDet (object detection), YuNet, S-Face (face detection & recognition), MiDaS / Depth Anything V2 (depth estimation)</p>
<p class="project-text"><strong>Hardware:</strong> ESP32-CAM (~₹800 / ~$10), Consumer PC / edge device</p>
<p class="project-text"><strong>System Components:</strong> Wireless video streaming (HTTP), Parallel inference pipelines, Audio alert system, Visual debugging overlays</p>

<h4 class="project-subtitle">Evaluation & Validation</h4>
<p class="project-text">ORCA was tested across indoor and outdoor environments, focusing on practical usability rather than benchmark chasing.</p>
<p class="project-text">Approximate observed performance:</p>
<ul class="project-list">
<li><strong>Object detection:</strong> ~93% mAP (COCO classes)</li>
<li><strong>Face detection:</strong> ~90% accuracy</li>
<li><strong>Face recognition:</strong> ~87% accuracy (local dataset)</li>
<li><strong>Depth estimation error:</strong> ±10 cm (0.5–3 m range)</li>
<li><strong>End-to-end throughput:</strong> ~8 FPS</li>
</ul>
<p class="project-text">These results validated the feasibility of a low-cost, multi-purpose wearable vision aid, though not production readiness.</p>

<h4 class="project-subtitle">Reflections & Learnings</h4>
<p class="project-text">This project marked an important transition from single-model demos to system-level computer vision engineering.</p>
<p class="project-text">Key takeaways:</p>
<ul class="project-list">
<li>Multi-model systems introduce orchestration and latency challenges not visible in notebooks</li>
<li>Hardware constraints force better architectural decisions</li>
<li>Real-time CV is about trade-offs, not just accuracy</li>
<li>Assistive technology requires reliability and interpretability, not just performance</li>
</ul>
<p class="project-text">ORCA also revealed gaps — configuration management, logging, and abstraction — which directly informed the design of later projects like Tesseract and MÍMIR.</p>

<h4 class="project-subtitle">Limitations & Future Directions</h4>
<p class="project-text"><strong>Current limitations:</strong> Performance degrades in low-light conditions, Depth estimation adds significant latency, Configuration is partially hard-coded</p>
<p class="project-text"><strong>Future improvements:</strong> Unified config and logging layers, Better low-light preprocessing, Spatial audio feedback, More capable edge hardware (Jetson / Raspberry Pi)</p>

<h4 class="project-subtitle">Status</h4>
<p class="project-text"><strong>Status:</strong> Completed (College Research Project)</p>
<p class="project-text"><strong>Nature:</strong> Experimental research & prototyping</p>
<p class="project-text"><strong>Outcome:</strong> Demonstrated feasibility of a low-cost, multi-purpose computer vision glasses aid</p>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>

<!-- Project 6: Vulkyrie v1 - Edge-Deployed Carcass Contamination Detection System -->
<div class="jumbotron project-card completed" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div class="image-carousel">
<img src="{{ '/images/vulkyrie 1.png' | relative_url }}" class="main-image" alt="Vulkyrie edge-deployed contamination detection system">
<div class="thumbnail-controls">
<button class="thumbnail-arrow prev" disabled>‹</button>
<div class="thumbnail-scroll">
<img src="{{ '/images/vulkyrie 1.png' | relative_url }}" class="thumbnail active" alt="Vulkyrie edge-deployed contamination detection system">
<img src="https://picsum.photos/seed/vulkyrie2/50/38.jpg" class="thumbnail" alt="Vulkyrie project screenshot 2">
<img src="https://picsum.photos/seed/vulkyrie3/50/38.jpg" class="thumbnail" alt="Vulkyrie project screenshot 3">
<img src="https://picsum.photos/seed/vulkyrie4/50/38.jpg" class="thumbnail" alt="Vulkyrie project screenshot 4">
</div>
<button class="thumbnail-arrow next">›</button>
</div>
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">College Project & Hackathon</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">Edge-deployed contamination detection system</p>
</div>
<div class="col-md-8">
<div class="card-content">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">Vulkyrie v1 — Edge-Deployed Carcass Contamination Detection System</h3>
<div style="margin-bottom: 15px;">
<p class="project-text">Low-cost, field-deployable computer vision and IIoT system for chemical contamination screening using color-based sensing and lightweight machine learning.</p>
</div>

<h4 class="project-subtitle">Context & Motivation</h4>
<p class="project-text">Monitoring chemical contamination in the field is often expensive, slow, and inaccessible, especially in rural or resource-constrained environments. In India alone, laboratory-grade chemical tests can cost ₹600–₹1200 ($7–$15) per sample, making large-scale or frequent monitoring impractical.</p>
<p class="project-text">Vulkyrie was built as a low-cost, field-deployable computer vision and IIoT system to explore whether color-based sensing + lightweight machine learning can serve as an early-warning mechanism for chemical contamination.</p>
<p class="project-text">This project specifically targeted Diclofenac contamination, a chemical linked to a catastrophic 99% collapse of India's vulture population, with severe downstream ecological and public-health consequences. The goal was not laboratory-grade accuracy, but rapid, accessible, and scalable screening.</p>

<h4 class="project-subtitle">Problem Statement</h4>
<p class="project-text">How can chemical contamination be detected cheaply, in real time, and in the field, using hardware that costs under ₹1000 ($12–$15), while still producing meaningful, interpretable outputs that can guide further investigation?</p>

<h4 class="project-subtitle">System Overview</h4>
<p class="project-text">Vulkyrie is an end-to-end research-to-prototype pipeline spanning:</p>
<ul class="project-list">
<li><strong>Data collection & chemical testing</strong></li>
<li><strong>ML model training and analysis</strong></li>
<li><strong>Edge deployment on ESP32 microcontrollers</strong></li>
<li><strong>IIoT-style backend aggregation</strong></li>
<li><strong>Geospatial visualization via a web dashboard</strong></li>
</ul>
<p class="project-text">Rather than focusing purely on model accuracy, the system emphasizes deployment realism, hardware constraints, and operational usability.</p>

<h4 class="project-subtitle">Key Engineering Decisions</h4>
<p class="project-text"><strong>Random Forest Regression</strong> was chosen over deep models due to:</p>
<ul class="project-list">
<li>Small, physically collected dataset</li>
<li>Interpretability of decision paths</li>
<li>Ease of pruning and simplification for microcontrollers</li>
</ul>

<p class="project-text"><strong>Edge-first design:</strong></p>
<ul class="project-list">
<li>Models were explicitly constrained to fit ESP32 memory and compute limits</li>
</ul>

<p class="project-text"><strong>Model-to-hardware translation:</strong></p>
<ul class="project-list">
<li>Full Random Forest logic was analyzed and reduced into threshold-based inference suitable for real-time execution</li>
</ul>

<p class="project-text"><strong>Data augmentation over brute-force modeling:</strong></p>
<ul class="project-list">
<li>Real chemical testing is costly and slow, so robustness was improved through controlled RGB noise injection</li>
</ul>

<p class="project-text"><strong>Centralized IIoT backend:</strong></p>
<ul class="project-list">
<li>Enables hotspot identification and geographic tracking of contamination events</li>
</ul>

<h4 class="project-subtitle">Computer Vision & ML Pipeline</h4>
<p class="project-text"><strong>Input:</strong> RGB values from TCS3200 color sensor</p>
<p class="project-text"><strong>Preprocessing:</strong></p>
<ul class="project-list">
<li>MinMax feature scaling</li>
<li>Noise-based data augmentation (3× expansion)</li>
</ul>
<p class="project-text"><strong>Model:</strong></p>
<ul class="project-list">
<li>Pruned Random Forest Regressor (n_estimators=15, max_depth=5)</li>
</ul>
<p class="project-text"><strong>Outputs:</strong></p>
<ul class="project-list">
<li>Continuous contamination concentration</li>
<li>Mapped to discrete risk levels: NEGATIVE, LOW, MODERATE, HIGH</li>
</ul>

<h4 class="project-subtitle">Edge Deployment & Hardware</h4>
<ul class="project-list">
<li><strong>Microcontroller:</strong> ESP32</li>
<li><strong>Sensor:</strong> TCS3200 color sensor</li>
</ul>
<p class="project-text"><strong>Inference:</strong></p>
<ul class="project-list">
<li>Simplified threshold logic derived from trained model</li>
</ul>
<p class="project-text"><strong>Interaction:</strong></p>
<ul class="project-list">
<li>Button-triggered sampling with reaction delay</li>
<li>Serial output for debugging and logging</li>
</ul>
<p class="project-text"><strong>Cost:</strong></p>
<ul class="project-list">
<li>One-time hardware cost: ₹800–₹1000 ($10–$12)</li>
<li>Per-test cost: ~₹100 (<$1)</li>
</ul>
<p class="project-text">This makes Vulkyrie orders of magnitude cheaper than conventional lab testing for preliminary screening.</p>

<h4 class="project-subtitle">Backend & Visualization</h4>
<ul class="project-list">
<li><strong>REST API:</strong> Node.js + Express</li>
<li><strong>Database:</strong> PostgreSQL for structured storage</li>
<li><strong>Frontend:</strong> React + Leaflet</li>
</ul>
<p class="project-text">Interactive geographic contamination maps, centralized monitoring of distributed devices. Designed to simulate IIoT-style deployment, not just a standalone device.</p>

<h4 class="project-subtitle">Evaluation & Validation</h4>
<p class="project-text"><strong>Dataset:</strong></p>
<ul class="project-list">
<li>24 physically collected chemical samples</li>
<li>Augmented to 72 samples for robustness</li>
</ul>
<p class="project-text"><strong>Evaluation focused on:</strong></p>
<ul class="project-list">
<li>Consistency under lighting variation</li>
<li>Stability of edge inference</li>
<li>Qualitative agreement with known concentration levels</li>
</ul>
<p class="project-text">The system is explicitly positioned as early-warning & screening, not definitive diagnosis.</p>

<h4 class="project-subtitle">Reflections & Learnings</h4>
<p class="project-text">This project was intentionally ambitious for its scope and constraints. The most challenging aspects were:</p>
<ul class="project-list">
<li>Real-world data collection involving chemical handling and concentration calibration</li>
<li>Translating an ML model into resource-constrained embedded logic</li>
<li>Understanding where engineering trade-offs matter more than raw model accuracy</li>
<li>Designing systems that actually run outside notebooks</li>
</ul>
<p class="project-text">Vulkyrie was also one of the first projects where I deeply engaged with:</p>
<ul class="project-list">
<li>Edge ML constraints</li>
<li>Model simplification strategies</li>
<li>Hardware–software co-design</li>
<li>End-to-end system thinking beyond model training</li>
</ul>

<h4 class="project-subtitle">Status</h4>
<p class="project-text"><strong>Stage:</strong> Experimental / Prototype</p>
<p class="project-text"><strong>Context:</strong> College project & hackathon prototype</p>
<p class="project-text"><strong>Readiness:</strong> Not production-grade, but technically validated</p>
<p class="project-text"><strong>Positioning:</strong> Proof-of-concept for low-cost, AI-assisted contamination screening</p>
</div>
<div class="see-more-container">
<div class="see-more-text">See more</div>
<div class="arrow-down"></div>
</div>
</div>
</div>
</div>

<!-- Project 7: Advait AI Community Platform -->
<!-- Commented out temporarily
<div class="jumbotron project-card ongoing" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div style="width: 100%; height: 300px; background-color: #e9ecef; border-radius: 8px; display: flex; align-items: center; justify-content: center; color: #6c757d; font-size: 0.8rem; margin-bottom: 15px;">
[Advait Community]
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">2025 - Present</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">300+ member AI community leadership and platform</p>
</div>
<div class="col-md-8">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">Advait AI Community Platform</h3>
<div style="margin-bottom: 15px;">
<p style="font-size: 0.9rem; line-height: 1.5; color: #444; margin-bottom: 12px;">Leading a 300+ member student-led AI community focused on research-oriented machine learning, systems engineering, and applied AI development. Building both the community culture and technical infrastructure.</p>
</div>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Leadership & Platform Development:</h4>
<ul style="padding-left: 20px; margin-bottom: 15px; font-size: 0.85rem; line-height: 1.5; color: #444;">
<li><strong>Community Building:</strong> Grew from 50 to 300+ members through strategic outreach and engagement.</li>
<li><strong>Technical Initiatives:</strong> Designed and coordinated research projects across LLMs, CV, and ML systems.</li>
<li><strong>Event Management:</strong> Organized technical talks, workshops, and internal study groups.</li>
<li><strong>Mentorship Programs:</strong> Established peer-to-peer learning and skill development frameworks.</li>
<li><strong>Platform Infrastructure:</strong> Built systems for project management, communication, and resource sharing.</li>
</ul>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Impact & Outcomes:</h4>
<p style="font-size: 0.85rem; color: #444; margin-bottom: 0;">20+ active projects, 15+ technical events, 300+ engaged members, strong industry partnerships</p>
</div>
</div>
</div>
-->

<!-- Project 7: ML Systems Research Pipeline -->
<!-- Commented out temporarily
<div class="jumbotron project-card ongoing" style="margin-top: 30px; margin-bottom: 30px; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
<div class="row">
<div class="col-md-4">
<div style="width: 100%; height: 300px; background-color: #e9ecef; border-radius: 8px; display: flex; align-items: center; justify-content: center; color: #6c757d; font-size: 0.8rem; margin-bottom: 15px;">
[Research Pipeline]
</div>
<p style="color: #6c757d; font-size: 0.8rem; margin-bottom: 10px; text-align: center;">2024 - Present</p>
<p style="font-size: 0.85rem; font-style: italic; color: #555; text-align: center; margin-bottom: 0;">Production-grade ML research and experimentation pipeline</p>
</div>
<div class="col-md-8">
<h3 style="font-weight: 700; font-size: 1.4rem; color: #222; margin-bottom: 12px;">ML Systems Research Pipeline</h3>
<div style="margin-bottom: 15px;">
<p style="font-size: 0.9rem; line-height: 1.5; color: #444; margin-bottom: 12px;">Developed a comprehensive pipeline for reproducible ML research that bridges academic experimentation with production deployment. Focuses on systematic experimentation and reliable results.</p>
</div>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Pipeline Components:</h4>
<ul style="padding-left: 20px; margin-bottom: 15px; font-size: 0.85rem; line-height: 1.5; color: #444;">
<li><strong>Experiment Tracking:</strong> Comprehensive logging of hyperparameters, metrics, and artifacts.</li>
<li><strong>Version Control:</strong> Model and dataset versioning with proper lineage tracking.</li>
<li><strong>Automated Testing:</strong> Unit tests, integration tests, and regression testing for ML components.</li>
<li><strong>Deployment Pipeline:</strong> CI/CD integration with automated model validation and deployment.</li>
<li><strong>Monitoring & Observability:</strong> Real-time performance tracking and drift detection.</li>
</ul>
<h4 style="font-weight: 600; font-size: 1.1rem; color: #222; margin-bottom: 10px; font-style: italic;">Technical Stack:</h4>
<p style="font-size: 0.85rem; color: #444; margin-bottom: 0;">MLflow, Docker, Kubernetes, GitHub Actions, Prometheus, Grafana, PyTorch</p>
</div>
</div>
</div>
-->
